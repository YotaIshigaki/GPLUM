
Dear Dr. Bronis de Supinski

In response to the reviewers' report, we have revised our manuscript
and we resubmit our paper entitled "Implementation and Performance of
Barnes-Hut N-body algorithm on Extreme-scale Heterogeneous Many-core
Architectures". In the following, we describe changes we made in the
form of the reply to reviewers' comments.

Yours Sincerely,
Masaki Iwasawa
Daisuke Namekata
Ryo Sakamoto
Takashi Nakamura
Yasuyuki Kimura
Keigo Nitadori
Long Wang
Miyuki Tsubouchi
Jun Makino
Zhao Liu
Haohuan Fu
Guangwen Yang


To reviewer 1

> Reviewer: 1
>
> Comments to the Author
> In this paper, the authors develop their parallel Barnes-Hut tree code
> for two heterogeneous architectures, Sunway TaihuLight and PEZY-SC2.
> The work presents new algorithms to achieve high performance for such
> currently uncommon architectures and give great performance,
> and is deserving of publication in IJHPCA. However, underlying
> ideas and results are not well presented. Addressing the
> following issues are necessary.
>
>

Thank you for your careful reading and detailed comments on our
manuscript. In the following, we describe changes we made.

> Major comments: Section 3: It is hard to understand the
>specifications of the system used in this paper. Please use a table
>to explain the features of the three systems.

We added a new table, table 1, in section 3, in which we summarize the
features of systems.

> Section 3, last paragraph: "Just one of these ... previously known
> parallelization algorithms". For item number 1, 3, 4, and 5, I agree
> with this statement. Regarding 2, the well known algorithm by Barnes
> 1990, which is used in the author's code, can significantly reduce
> the required memory B/F if the cache memory size is enough. For the
> application (Saturn's ring) in this paper, how severe is very small
> memory B/F?

As you pointed out, by using the algorithm developed by Barnes, the
required memory B/F is reduced compared with the original tree
algorithm. In the original algorithm, the required B/F is more than
unity. On the other hand if we use Barnes algorithm and the
interaction list is shared by a few hundred particles, the dominant
procedure of memory access is the tree construction, in which all
particle data is read or written about 50 times per step. The typical
size of a particle is about 64B, therefor the amount of memory access
per step per particle is roughly 3200. For the force calculation in
the Saturn's ring simulation, the size of the interaction list is
about 2000 and the number of operations per interaction is about
40. Thus the total number of floating operations per step per particle
is 80000. Considering the random nature of the memory access for tree
construction, even if we use the Barnes algorithm, the required B/F
sould be much greater than 0.04. This value is large on the system we
used in this paper. We added these discussions in section 3.


> Section 4.2: the persistent interaction list method. This method
> should give additional force errors depending on the distribution of
> particles. To reduce the error, are there any differences in time
> stepping and tree opening criterion between with and without sharing
> interaction lists over multiple timesteps?

We thank you very much for raising this point. As you pointed out, the
persistent interaction list method gives the additional force error
from the deformation of the tree cells due to shear motion.We found
that the time integration error grows rather rapidly because the force
error is correlated with time when the same interaction list is used,
and because the error due to shear deformation of tree cells grows
linearly in time. As a result, the error, such as the error of the
total energy, grows as square of time, while the same interaction list
is used (please see figure 6 in section 6.1).

To reduce this integration error, we developed a new method in which
the interaction list is constructed at the midpoint of the period for
which the same interaction list is used. For example, if we use the
same interaction list for 64 steps, we construct the interaction list
by using the predicted positions of particles at 32 steps after the
current time. With this algorithm, the shear deformation of cells in
first half of the reusing period and that in the last half are in
opposite directions, and we can expect that the dominant error term
would cancel out. We implemented this method to our code and confirmed
that this algorithm actually cancels the dominant error (please see
figure 6 in section 6.1). As a result, by using this method we do not
have to make timestep or opening criterion small.

Unfortunately, we did not use this method for the performance
measurements. However, the additional cost is very small because
particles are almost on the circular orbits and the prediction is
easy. Thus even if we use this method, the performance results would
not change. We added above descriptions in sections 4.2 and 6.1.

> Figure 3:
> Why do time per one step on TaihuLight and Shoubu-B show up-down?
>

This up-down is caused by the sudden change in the size of the
particle group for Barnes' algorithm. Since the distribution of
particles is almost uniform, all tree cells in the same level have
nearly the same number of particles.  When we apply Barnes' algorithm,
we set the maximum number of particles in the group.  The actual
number is the maximum number of particles in tree cells, which does
not exceed the limit.  For example, when we set this limit to 1024,
actual value can be anywhere between 1024 and 256. If it is, say, 300
for certain value of the total number of particles, when we double the
number of particles, the actual value would become 600, and when we
double the number of particles again, the actual value would go back
to 300 since we go down the tree one level. Thus, the size of particle
group to share the interaction list shows oscillation as the function
of the total number of particles, and that is the reson for the
oscillation of the calculation time. We added these descriptions in
section 5.2.


>
> Minor comments:
>
> Abstract: "global" simulation
> The meaning of "global" is unclear.
>

We meant whole ring simulation by "global" simulation. We clearly
mentioned this point in abstract.


> Section 1 5th paragraph: "OpenACC results in rather poor performances
> in the case of TaihuLight".
> References are necessary.
>

Thank you for your comments. We added a reference, Cai et al.(2018),
in 5th paragraph in section 1.


> "and thus the current system ... Xeon-D CPU".
> Typo, without a verb.
>

Thank you for finding typo. We corrected it.

> Section 2, 1st paragraph: Clarify the meaning of "more Centaurs".
>

Centaurs are minor planets between Saturn and Uranus. We added this
description in section 2.


> Section 2, 6th paragraph: "Almost all previous ... only for 10
> orbital periods".  This paragraph and the last two paragraphs are
> near duplicates.

Thank you for your comments. We combined these paragraphs into one
paragraph.


> Section 2, 7th paragraph: "or around 10^12 particle-steps".
> I do not understand where the number 10^12 comes. Do the authors
> implicitly assume 1,000 steps are necessary for an orbit? If so, please
> specify.
>

Thank you for your comments. Yes, we meant 1,000 steps are necessary
for an orbit. We added this description in section 2.


> Section 4.2, 2nd paragraph: "However, on almost all modern implementation".
> References are necessary.
>

Thank you for your comments. We added the references, Bedorf et
al.(2014) and ishiyama et al. (2012), in this sentence.


> Section 4.5, 3rd paragraph:
> How can this algorithm reduce the communication cost? Please describe
> a typical reduction rate.
>

For many supercomputers, MPI_Alltoall is mush slower than
MPI_Allgather because the performance of the former is limited by the
bisection bandwidth and that of the latter is limited by the injection
bandwidth. For example, on TaihuLight, the time for MPI_Alltoall with
a short message size among 10000 processes can be as much as several
minutes. On the other hand, that for Allgather is about ten
millisecond. Thus the reduction rate is roughly 10000. Thus to avoid
to use MPI_Alltoall is necessary. We added these descriptions in
section 4.5.


> Section 4.6, item number 3: Typo "to the the CPE". .
>

Thank you for your comments. We corrected it.


> Section 4.7, 1st paragraph: "does not give very good performance".
> It would be helpful to mention the performance qualitatively.
>

In the compiler generated code, there is no loop unrolling and
scheduling is not good. As a results, with the compiler-generated code
the achieved efficiency was around 10%. We added these description in
section 4.7.


> Section 5, last paragraph:
> Why is this statement made here, what does it mean? What is your intent to compare with
> results from old and "smaller" supercomputers?
>

In this paragraph, we would like to stress that on modern, "bigger"
and low B/F supercomputers, we can achieve the performance efficiency
almost same as that have been achieved on old, "smaller" and high B/F
supercomputers, by using our algorithms.


> Figure 5: It would be helpful to plot lines showing idealized scaling.
>
Thank you for your comments. We added the line showing idealized
scaling in figure 5.


> Section 5.1, 3rd paragraph: "For particle-tree-node, interaction, we
> used center-of-mass approximation".
> When higher-order terms (e.g., quadrupole) are calculated, what
> performance gain should be expected?
>

In our simulations, the energy error for the run with center of mass
approximation (effectively dipole moment) with theta=0.5 is almost the
same as that for the run with up to quadrupole moment with
theta=1.0. If we use theta=1.0, the size of the interaction list per
particle is reduced by 30% compared with the case of theta=0.5 and the
time to solution would be reduced by 30 %. However, even if we use
theta=1.0, the size of the interaction list is enough long so that the
processors are always busy. Thus the performance would not be
changed. We added these descriptions in section 5.1


> Section 6, 3rd paragraph:
> What does "Athread call" mean?
>

Atherad is a thread library to use CPEs in parallel. We added this
description in section 6.

To Reviewer 2

> Reviewer: 2
>
> Comments to the Author
> This paper is basically well written and presents useful information on the parallel Barnes-Hut algorithm on heterogeneous many-core systems (such as software frameworks, performance measurements, and new algorithms).

Thank you for your careful reading and detailed comments on our
manuscript. In the following, we describe changes we made.

>
> However, I have several concerns.
>
> (1) Section 4.1 introduces several new algorithms. However, their individual effects (improvements) or even combined effects are not shown quantitatively in the paper except for "less than 1%" in Section 4.2. For example, in Section 4.5, "the reduction ... was quite significant" is not a quantitative evaluation.
>

In the following, we quantitatively describe individual effects of the
methods described in section 4.

* For using cylindrical coordinate (section 4.3).

  The amount of data size of communication is roughly proportional to
  the surface areas of domains. For example, if we use 160K (5x32000)
  nodes, the total surface area of domains in Cartesian coordinate is
  about 20 times larger than that in Cylindrical coordinate. Thus the
  amount of data size of the communication is reduced by a factor of
  20. We added these descriptions in section 4.3.

* For coordinate rotation (section 4.4).

  In our simulation, each particle rotates for about 0.4 radian in the
  theta direction during the 64 steps.  If we use 160K (5x32000)
  nodes, without coordinate rotation, all particles in all nodes need
  to be sent to other nodes. However, in the rotating frame, particles
  rotate very slowly. Even the fastest particles rotates by only 2e-4
  radian in the theta direction during 64 steps. As a result, by using
  coordinate rotation method, only 5 % of particles need to be sent to
  other domains. Thus we can reduce communication cost by a factor of
  20. We added these descriptions in section 4.4.


* Elimination of all-to-all communication (section 4.5)

  For many supercomputers, MPI_Alltoall is mush slower than
  MPI_Allgather because the performance of the former is limited by the
  bisection bandwidth and that of the latter is limited by the injection
  bandwidth. For example, on TaihuLight, the time for MPI_Alltoall with
  a short message size among 10000 processes can be as much as several
  minutes. On the other hand, that for Allgather is about ten
  millisecond. Thus the reduction rate is roughly 10000. Thus to avoid
  to use MPI_Alltoall is necessary. We added these descriptions in
  section 4.5.




* Load balance among computing cores (section 4.6)

  Without this method, the typical variation of the force calculation
  cost among all CPEs is 40%. On the other hand, with this method, the
  variation is a few percent. As a result, by using this method, the
  time for the force calculation is reduced by 10%. We added these
  descriptions in section 4.6.

* Interaction Kernel (section 4.7)

  In the compiler generated code, there is no loop unrolling and the
  instruction scheduling is not ideal. As a results, with the
  compiler-generated code the achieved efficiency around 10%. Thus our
  kernel written in the assembly language is five times faster than
  the kernel generated by the compiler. We added these descriptions in
  section 4.7.


> (2) It is unclear how significantly persistent interaction lists make the calculation inaccurate compared to the pre-existing Barnes-Hut approximation. For example, if we need to make interaction lists 20% long in order to achieve the equivalent degree of approximation, the 20% increased floating operations are just overheads.


We thank you very much for raising this point. As we will describe
below, we do not have to make the interaction list long by using new
method to reduce the additional error generated by the persistent
interaction list method.

As you pointed out, the persistent interaction list method gives the
additional force error from the deformation of the tree cells due to
shear motion. We found that the time integration error grows rather
rapidly because the error due to shear deformation of tree cells grows
linearly in time. As a result, the error, such as the error of the
total energy, grows as square of time, while the same interaction list
is used (please see figure 6 in section 6.1).

To reduce this integration error, we developed a new method in which
the interaction list is constructed at the midpoint of the period for
which the same interaction list is used. For example, if we use the
same interaction list for 64 steps, we construct the interaction list
by using the predicted positions of particles at 32 steps after the
current time. With this algorithm, the shear deformation of cells in
first half of the reusing period and that in the last half are in
opposite directions, and we can expect that the dominant error term
would cancel out. We implemented this method to our code and confirmed
that this algorithm actually cancels the dominant error (please see
figure 6 in section 6.1). As a result, by using this method we do not
have to make timestep or opening criterion small.

Unfortunately, we did not use this method for the performance
measurements. However, the additional cost is very small because
particles are almost on the circular orbits and the prediction is
easy. Thus even if we use this method, the performance results would
not change. We added above descriptions in sections 4.2 and 6.1.


>Note that Hamada et al. 2009 seems not to count increased particle-particle interactions for GPU (compared to the corresponding particle-cell interactions for CPU) for their title's "42 TFlops".

Thank you for your comments. We also added the performance when the
interaction list is not shared for multiple particles (i.e. the number
of interactions is minimum) in section 5.2. In this case, the number
of interactions is reduced by 25% and the performances are 36, 8 PF
and 0.75 PF on TaihuLight, Gyoukou and Shoubu System B.


> (3) In Section 4.7, it is unclear how significantly the use of single precision operations make the calculation inaccurate compared to the pre-existing Barnes-Hut approximation.
>

According to McMillan and Aarseth (1993), the relative force error by
introducing center-of-mass approximation (effectively dipole moment
approximation) with theta=0.5 is about 0.1%. This force error can be
expressed by 10 bits. Thus the error from using single precision
operations is much smaller than that from the Barnes-Hut
approximation. We added this point in section 4.7.



> (4) Since Section 5.1 shows that "For PEZY-SC2 based systems we used the same operation count as we used for TaihuLight", does this mean that performances such as 1.01PF and efficiency such as 35.5% are more than real values?

PEZY-SC2 has special function units (SFU) for reciprocal square root
operation and we used it for the force calculation. However, the
latency of SFU is very large. If we considered this latency, the
efficiency for PEZY-SC2 based system would increase by several
percent. We added these description in a footnote in section 5.1.
