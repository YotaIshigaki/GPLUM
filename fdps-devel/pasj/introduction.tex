In the field of computational astronomy, simulations based on particle
methods have been widely used. In such simulations, a system is either
physically a collection of particles as in the case of star clusters,
galaxies and dark-matter halos, or modeled by a collection of
particles, as in SPH (smoothed particle hydrodynamics) simulation of
astrophysical fluids.  Since particles move automatically as the
result of integration of the equation of motion of the particle,
particle-based simulations have an advantage for systems experiencing
strong deformation or systems with high density contrast.  This is one
of the reasons why particle-based simulations are widely used in
astronomy. Examples of particle-based simulations include cosmological
simulations or planet-formation simulations with gravitational
$N$-body code, simulations of star and galaxy formation with the SPH
code or other particle-based codes, and simulations of planetesimal
formation with the DEM (discrete element method) code.

We need to use a large number of particles to improve the resolution
and accuracy of particle-based simulations, and in order to do so we
need to increase the calculation speed and need to use
distributed-memory parallel machines efficiently. In other words, we
need to implement efficient algorithms such as the Barnes-Hut tree
algorithm \citep{1986Natur.324..446B}, the TreePM
algorithm \citep{1995ApJS...98..355X} or the Fast Multipole Method
\citep{2000ApJ...536L..39D} to distributed-memory parallel computers.
In order to achieve high efficiency, we need to divide a computational
domain into subdomains in such a way that minimizes the need of
communication between processors to maintain the division and to
perform interaction calculations. To be more specific, parallel
implementations of particle-based simulations contain the following
three procedures to achieve the high efficiency: (a) domain
decomposition, in which the subdomains to be assigned to computing
nodes are determined so that the calculation times are balanced, (b)
particle exchange, in which particles are moved to computing nodes
corresponding to the subdomains to which they belong, and (c)
interaction information exchange, in which each computing node
collects the information necessary to calculate the interactions on
its particles.  In addition, we need to make use of multiple CPU cores
in one processor chip and SIMD (single instruction multiple data)
execution units in one CPU core, or in some cases GPGPUs
(general-purpose computing on graphics processing units) or other
accelerators.


%% In order to improve the resolution and accuracy of particle-based
%% simulations, it is necessary to utilize present-day HPC systems.
%% However, to develop a calculation code for particle systems which can
%% achieve high efficiency on present-day HPC systems is difficult and
%% time-consuming. There are several reasons for this difficulty. In
%% order to achieve high efficiency, we need to decompose the
%% computational domain, assign subdomains to computing nodes, and
%% redistribute particles according to their positions. This
%% decomposition should be dynamically changed to guarantee the good load
%% balance. This division of the computational domain means that
%% computing nodes need to exchange the information of particles to
%% evaluate the interactions on their particles. To achieve high
%% efficiency, the amount of the exchanged data must be minimized, and
%% interaction calculation should also be efficient, making good use of
%% cache memory and SIMD execution units. It is also important to make
%% use of GPGPUs or other types of accelerators, if available.

In the case of gravitational $N$-body problems, there are a number of
works in which the efficient parallelization is
discussed \citep{1994JCoPh.111..136S, 1996NewA....1..133D,
2004PASJ...56..521M, 2009PASJ...61.1319I,
Ishiyama:2012:PAN:2388996.2389003}.  The use of SIMD units is
discussed in \citet{2006NewA...12..169N}, \citet{2012NewA...17...82T}
and \citet{2013NewA...19...74T}, and GPGPUs
in \citet{2009NewA...14..630G}, \citet{hamada2009novel}, \citet{Hamada:2009:THN:1654059.1654123}, \citet{Hamada:2010:TAN:1884643.1884644}, \citet{2012JCoPh.231.2825B}
and
\citet{Bedorf:2014:PGT:2683593.2683600}.

In the field of molecular dynamics, several groups have been working
on parallel implementations. Examples of such efforts include
Amber \citep{2015AMBER}, CHARMM \citep{2009CHARMM},
Desmond \citep{GB14}, GROMACS \citep{2014GROMACS},
LAMMPS \citep{1995LAMMPS}, NAMD \citep{2005NAMD}. In the field of CFD
(computational fluid dynamics), Many commercial and non-commercial
packages now support SPH or other particle-based methods
(PAM-CRASH \footnote{https://www.esi-group.com/pam-crash},
LS-DYNA \footnote{http://www.lstc.com/products/ls-dyna},
Adventure/LexADV \footnote{http://adventure.sys.t.u-tokyo.ac.jp/lexadv})


Currently, parallel application codes are being developed for each of
specific applications of particle methods. Each of these codes
requires multi-year effort of a multi-person team. We believe this
situation is problematic because of the following reasons.

First, it has become difficult for researchers to try a new method or
just a new experiment which requires even a small modification of
existing large codes. If one wants to test a new numerical scheme, the
first thing he or she would do is to write a small program and to do
simple tests. This can be easily done, as far as that program runs on
one processor. However, if he or she then wants to try a
production-level large calculation using the new method, the
parallelization for distributed-memory machines is necessary, and
other optimizations are also necessary. However, to develop such a
program in a reasonable time is impossible for a single person, or
even for a team, unless they already have experiences of developing
such a code.

Second, even for a team of people developing a parallel code for one
specific problem, it has become difficult to take care of all the
optimizations necessary to achieve a reasonable efficiency on recent
processors. In fact, the efficiency of many simulation codes mentioned
above on today's latest microprocessors are rather poor, simply
because the development team does not have enough time and expertise
to implement necessary optimizations (in some case they require the
change of data structure, control structure and algorithms).

In our opinion, these difficulties have significantly slowed down
researchs in the numerical methods and also the research using
large-scale simulations.

%% Thus, to develop a code which has all of these necessary features for
%% present-day HPC systems has become a big project which requires
%% multi-year effort of a multi-person team. It has become difficult for
%% researchers outside such a team to try any new experiments which
%% requires nontrivial modification of the code. If one wants to develop
%% a new numerical scheme for particle-based simulation or to apply it to
%% new problem, it is necessary to write his/her own code. However, it is
%% practically impossible for a single person, or even for a group of
%% people, to develop a new code which can run efficiently on present-day
%% HPC systems in a reasonable time.  This difficulty, in our opinion,
%% has slowed down the evolution of the entire field of computational
%% science for the last two decades. Here we discussed the situation of
%% particle-based codes. The situation of grid-based codes is similar.

%% JM --- modified up to here as of Feb 8 0:46

We have developed FDPS (Framework for Developing Particle
Simulator)\footnote{https://github.com/FDPS/FDPS} \citep{2015FDPS} in
order to solve these difficulties. The goal of FDPS is to let
researchers concentrate on the implementation of numerical schemes and
physics, without spending too much time on parallelization and code
optimization. To achieve this goal, we separate a code into domain
decomposition, particle exchange, interaction information exchange and
fast interaction calculation using Barnes-Hut tree algorithm and/or
neighbor search and the rest of the code. We implement these functions
as ``template'' libraries in C++ language. The reason why we use the
template libraries is to allow the researchers to define their own
data structure for particles and their own functions for
particle-particle interactions, and to provide them with
highly-optimized libraries with small software overhead.  A user of
FDPS needs to define the data structure and the function to evaluate
particle-particle interaction. Using them as template arguments, FDPS
effectively generates the highly-optimized library functions which
perform complex operations listed above.

From users' point of view, what is necessary is to write the program
in C++, using FDPS library functions and to compile it using a
standard C++ compiler. Using FDPS, users thus can write their
particle-based simulation codes for gravitational $N$-body problem,
SPH, MD (molecular dynamics), DEM, or many other particle-based
methods, without spending their time on parallelization and complex
optimization. The compiled code will run efficiently on large-scale
parallel machines.

For grid-based or FEM (Finite Element Method) applications, there are
many frameworks for developing parallel applications. For example,
Cactus \citep{2003Cactus} has been widely used for numerical
relativity, and BoxLib \footnote{https://ccse.lbl.gov/BoxLib/} is
designed for AMR (adaptive mesh refinement). For particle-based
simulations, such frameworks have not been widely used yet, though
there were early efforts as in \citet{1995CoPhC..87..266W}, which is
limited to long-range, $1/r$ potential. More recently, LexADV\_EMPS is
currently being developed \citep{2015LexADV_EMPS}. As its name
suggests, it is rather specialized to the EMPS (Explicit Moving
Particle Simulation) method \citep{2014Murotani}.




In section~\ref{sec:user}, we describe the basic design concept of
FDPS.  In section~\ref{sec:implementation}, we describe the
implementation of parallel algorithms in FDPS. In
section~\ref{sec:performance}, we present the measured performance for
three astrophysical applications developed using FDPS. In
section~\ref{sec:performancemodel}, we construct a performance model
and predict the performance of FDPS on near-future supercomputers.
Finally, we summarize this study in section~\ref{sec:conclusion}.

%Some contents in sections \ref{sec:user}, \ref{sec:implementation} and
%4.1 have been published in \citet{2015FDPS}.

%In section~\ref{sec:performance}, we illustrate
%how FDPS actually simplifies the development of user programs, using
%three sample FDPS applications, and describe their performance. 

% LocalWords:  HPC subdomains SIMD GPGPUs multi FDPS parallelized SPH
% LocalWords:  parallelization
