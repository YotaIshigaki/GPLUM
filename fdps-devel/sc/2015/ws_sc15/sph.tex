In this section, we discuss the performance of an SPH simulation code
with self-gravity implemented using FDPS. The test problem used is the
simulation of Giant Impact (GI). The giant impact
hypothesis \cite{1975Icar...24..504H, 1976LPI.....7..120C} is one of
the most popular scenarios for the formation of the Moon. The
hypothesis is as follows. About 5 billion years ago, a Mars-sized
object (hereafter, the impactor) collided with the proto-Earth
(hereafter, the target). The collision scattered a large amount of
debris, which first formed the debris disk and eventually the
Moon. Many researchers have performed simulations of GI, using the SPH
method.
\cite{1986Icar...66..515B, 2013Icar..222..200C, 2014NatGe...7..564A}.

For the gravity, we used monopole-only kernel with $\theta=0.5$. We
adopt the standard SPH scheme
\cite{1992ARA&A..30..543M, 2009NewAR..53...78R, 2010ARA&A..48..391S}
for the hydro part. Artificial viscosity was used to handle shock waves
\cite{1997JCoPh.136..298M}, and 
the standard Balsara switch was used to reduce the shear viscosity
\cite{1995JCoPh.121..357B}. Assuming that the target and impactor
consist of granite, we adopt the equation of state of
granite \cite{1986Icar...66..515B} for the particles. The initial
conditions, such as the orbital parameters of the two objects, are the
same as those in \cite{1986Icar...66..515B}. In this paper, we report
the weak scaling performance with about 250k particles per node. For
the largest calculation, we used $1.0$ billion particles and $4096$
nodes.

Similarly to our $N$-body simulation code, we run our SPH simulation
code on K computer and Cray XC30. The maximum numbers of cores we used
are $32768$ cores ($4096$ nodes) on K computer, and $2064$ cores ($86$
nodes) on Cray XC30.

For gravity calculations, we used highly-optimized kernel developed
using SIMD builtin functions on both platforms. However, for fluid
calculations, we used the optimized kernel on the K computer, but do
not on Cray XC30.

\begin{figure}
  \begin{center}
%    \includegraphics[width=8cm]{fig/GI.eps}
    \includegraphics[width=7cm]{fig/GI.eps}
  \end{center}
  \caption{Temperature maps of the target and impactor in the run of
  $9.9$ million particles at four different epochs. }
  \label{fig:evolutionGI}
\end{figure}

Figure~\ref{fig:evolutionGI} shows the time evolution of the target
and impactor for a run with 9.9 million particles. We can see that the
shock waves are formed just after the moment of impact in both the
target and impactor (t=2050sec). The shock propagates in the target,
while the impactor is completely disrupted (t=2847sec) and debris is
ejected. Part of the debris falls back to the target, while the rest
will eventually form the disk and the Moon. So far, the resolution
used in the published papers have been much lower. We plan to use this
code to improve the accuracy of the GI simulations.

Filled and open squares in Figure~\ref{fig:benchdisk} show the
measured performance of our SPH simulation code. We can see the
measured efficiency and scalability are both very good on both
platforms, similarly to our $N$-body simulation code in the previous
section. On the K computer, efficiency is very close to 40\% for the
entire range of the number of nodes. The difference of SPH performance
between K computer and Cray XC30 is smaller than that of $N$-body
performance. This is because our SPH simulation code on Cray XC30 does
not efficiently utilize SIMD instructions for fluid
calculations. Although the SIMD instructions are used for gravity
calculations, the calculation cost is dominated by fluid calculations.

The largest number of particles used for GI simulations so far
reported is 100 million \cite{2014LPI....45.2703T}. Unfortunately,
performance numbers are not given. After we replace the interaction
kernels with SIMD-optimized ones for hydrodynamics part, we believe we
can achieve the performance not so much lower than that we achieved
for pure gravity calculation.

% LocalWords:  SPH FDPS impactor proto Balsara rr rrr Grav Wendland SIMD

% LocalWords:  builtin Wallclock timestep wallclock monopole
