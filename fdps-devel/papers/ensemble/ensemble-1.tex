%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% 分子シミュレーション研究会会誌「アンサンブル」用テンプレートファイル
%
% Ver. 3.0  2016/07/08
%
%% 1) イントロダクション+説明
%%     概要 考え方
%%     こんなコードはこう動く的サンプル紹介
%%     LJ のコード？
%%     内部構造、ツリーとかの話をちょっとする？
%%     手法を開発する人に使って欲しいみたいな話を。
%% 2)  使い方の詳しい話
%%     C++ の紹介+
%%     F2003 もある!
%%     ユーザーコードの書き方
%%     内部構造、ツリーとかの話を詳しく？
%% 3)  MD スペシフィックな
%%     ボンド
%%     周期境界
%% 4)  チューニング
%%     大規模実行
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[twocolumn,10pt]{jarticle}
\usepackage[review]{ensemble}
%
% === 使用方法 ===
%
% \usepackage[option]{ensemble}
%
% option のところを，記事の種類に合わせて変更してください．
% 
% review : 論文・研究紹介（著者一人）
% reviewb : 論文・研究紹介（著者二人）
% intro : 特集の前書き，研究室紹介，海外紹介（著者一人）
% introb : 特集の前書き，研究室紹介（著者二人）
% report : 夏の学校報告等（著者有り、著者紹介無し）
% noauther : 報告（著者無し）
%
% amsmath, amssymb, cite, color, graphicx, here, 
% は，ensemble.sty の中で読み込まれています．
%
\setcounter{page}{101}  % 開始ページ (編集委員用)

\title{% タイトル
[]連載 FDPS入門 (1) 
}

\author{% 著者名
牧野淳一郎
}

\email{% e-mail address
jmakino@people.kobe-u.ac.jp
}

\affiliation{% 所属
神戸大学\quad 理学研究科/理研 計算科学研究機構
}

\career{% 著者紹介
牧野淳一郎(学術博)：〔経歴〕1989年東京大学大学院総合文化研究科博士課程修了，同年東京大学大学教養学部助手．2016年から現所属．〔専門〕天体物理、計算科学．\\
}

%% % 第2著者
%% \authorb{原子次郎}
%% \emailb{genshi@aaa.bbb.ccc}
%% \affiliationb{% 所属
%% 原子大学\quad シミュレーション学部
%% }
%% \careerb{% 著者紹介
%% 原子次郎(博士(工学))：〔経歴〕1985年原子科学大学理工学研究科博士課程修了，同年原子科学研究所に入所．1995年から現所属．〔専門〕原子力工学〔趣味〕演劇鑑賞．\\
%% 写真サイズは縦35mm横25mm程度．
%% }

\abst{% 概要
本稿では、私達が開発・公開している多体シミュレーション
プログラム開発フレームワーク「FDPS (Framework for Developping Particle
  Simulators)」を紹介します。FDPS は、粒子シミュレーションを研究に使っ
ている多くの研究者が、並列化や計算機アーキテクチャ固有のチューニングに
多大な時間を費やすことなく、自分の扱いたい問題向けのシミュレーションプ
ログラムを容易に作成できるようになることを目標として開発したフレームワー
クです。連載第一回の今回は、FDPS の開発の背景、考え方と、実際にどのよ
うなことができるか、という簡単な例を紹介します。\\


}

\keyword{
粒子シミュレーション、分子動力学、HPC、並列化、MPI\\
}

% 顔写真のファイル名．デフォルトはphoto.eps, photob.eps
% 第1著者
\photofile{makino.eps} 
% 第2著者
%\photofileb{photob.eps} 

%------------------------------------------------------------------------
% ユーザー定義のマクロはここに書く．
%------------------------------------------------------------------------
%\renewcommand{\v}[1]{{\bf #1}}
%\newcommand{\ave}[1]{\left< #1 \right>}
%------------------------------------------------------------------------
\begin{document}
\maketitle
%------------------------------------------------------------------------
% 原稿ここから
%------------------------------------------------------------------------

\section{はじめに}

今号から 4 回にわたって、私達が開発・公開している多体シミュレーション
プログラム開発フレームワーク「FDPS (Framework for Developping Particle
  Simulators)」\cite{FDPS}を紹介します。第一回となる本稿では、
\begin{itemize}
\item FDPS を開発している目的、背景はなにか
\item FDPS は具体的には何をしてくれるか
\end{itemize}  
といったあたりを紹介しようと思います。
\\

\section{FDPS 開発の背景と動機}

\subsection{背景}

まず、背景はなにか、です。これは、現在、大規模な粒子シミュレーションを
する、特に、そのためのプログラムを開発することが非常に大変になってい
る、ということです。30年前であれば、粒子シミュレーションをしよう、と思っ
た時に、自分でプログラムを書くことはそれほど大変ではありませんでした。
普通に書いた上で、チューニングといえばベクトル化程度だったからです。

ところが、現在では、「普通にプログラムを書く」のではどうにもならなくなっ
ています。
まず、1台
のデスクトップPCや Intel CPU がのったサーバーでプログラムを走らせるに
しても、CPU コアが複数あり、またそのコアの中には複数の演算を並列に行うSIMD
ユニットがあるため、これらを有効に使うかどうかで性能が数十倍変わります。
Intel の Xeon Phi CPU では、CPU コアが60個程度あり、またその中に、
単精度だと16演算を並列に行うSIMD演算ユニットが2つありますから、これらを有
効に使うかどうかで最大1000倍におよぶ性能差が生じることになります。

昔だってスパコンを使うならチューニングとかベクトル化とかしたものだよ、
と30年前に現役だった読者の方は思うかもしれませんが、現代における
「並列化とか高度なチューニング」は当時の「ベクトル化」とは全く比較にな
らない、はるかに大変な作業になっています。

大変になっている要因を整理してみると、次のようにまとめられます。

\begin{itemize}

\item 並列化の階層の増加

\item メモリボトルネックの発生とそれには対応したメモリ階層の増加・複雑化

  
\end{itemize}  

以下、それぞれについて簡単に述べます。

現代の典型的な大規模並列計算機は、

\begin{itemize}

\item 複数の演算を並列に行う SIMD 演算ユニットを (コア内SIMD)
\item 複数もつ演算コアを (スーパースカラー)
\item 複数もつプロセッサチップを (マルチコア)
\item 複数ネットワークで接続した (分散メモリ並列)

\end{itemize}  
構成を持っています。つまり、4つの違うレベルで並列動作する複数のユニッ
トを持っています。しかも、それぞれのレベルで、どういうふうにすれば上手
く並列化できるか、が違います。この中で、「スーパースカラー」というのは、
CPU が、機械語プログラムの中から、並行して実行できそうなものを同時に
実行する機能です。なので、これは
ある程度ハードウェアがやってくれます。しかし、他の3つのレベルのために固有の
最適化が必要で、しかも分散メモリ並列では MPI を使ってプログラム全体を
書換え、例えば空間分割をして粒子を移動させるといった処理も書く必要がで
てきます。

並列化の複数のレベルの扱いをさらに困難なものにしているのが、メモリ階層
の存在です。1970年代から80年代前半にかけてのベクトル計算機は、半導体メ
モリが磁気コアメモリにとってかわった時代のものであり、演算器に対して十
分な速度でデータを供給することができました。しかし、実はこん
なことができたのは歴史的にはこの時期と、計算機の黎明期である1950年代だ
けで、それ以外では常に、演算器のほうがメモリより速く、演算器の性能を生
かすためには容量は小さいけれど高速なキャッシュメモリを使っています。

しかし、この、キャッシュメモリは、並列計算機とは決して相性のいいもので
はありません。マルチコアプロセッサでは、それぞれのコアが独立にキャッシュ
をもちたいわけですが、そうすると、あるコアがメモリのどこかに書いても、
その同じアドレスのデータを別のコアが自分のキャッシュにもっているとか、
あるいはまだ誰かのキャッシュの中でしか更新されていなくて主記憶には古い
データがあるものを主記憶から読んだ時に新しいデータにならないといけないといった
問題があります。こういったややこしい状況でもちゃんと整合性がある結果を
保証するのが「コヒーレントキャッシュ」というもので、そのためにコア間で
複雑なやりとりをする大規模なハードウェアが必要です。

これは必要なものではありますが、性能を出すためにはいかにしてキャッシュを制
御するか、が重要になり、キャッシュの特性を考慮したプログラムを書く必要
がでてきます。最近のプロセッサでは3レベルから4レベルのキャッシュを持つ
ようになっています。しかし、密行列乗算ならともかく、それ以外の計算アル
ゴリズムではこのような複数レベルのキャッシュを有効に使えるとは限らないで
すし、できたとしてもプログラムは極めて複雑なものにならざるを得ません。

このため、単純なアルゴリズムでも、最新の高性能プロ
セッサで性能を出すのは容易なことではなくなっています。

実際、今この文章を読んでおられる読者の皆様の中に、俺はMPIで並列化して
キャッシュも有効利用しSIMD演算器も使って FMM や PME を実装して高速な
並列MDプログラムを書いた、あるいは書ける、あるいは書く気がある、という
人はあまり多くないのでは、と思います。

%% 計算機1台とか、GPGPU がついた計算サーバ1台であれば、プログラム書き、
%% チューニングして速くしていく、というのもできなくはありません。しかし、
%% MPI での並列化を効率的に行う、となると非常にハードルが高くなります。
%% 計算ノードの数が少なければ、各ノードが全原子のコピーを持ち、分担を決め
%% て自分の担当の原子をアップデートし、各ノードが自分がアップデートした
%% 原子を放送する、といった単純な方法でも性能が向上します。しかし、この方
%% 法では、ノード数が増えると計算時間は速くなるのに通信時間は減らないので、
%% 性能向上に限界があります。


\subsection{困難を減らす方法}

では、どうすればいいのか？というのがここでの問題です。多くの場合にとら
れているアプローチは、大規模なソフトウェアを開発チームを作ることでなん
とか開発しよう、というもので、実際、様々な応用分野で、粒子法の並列化さ
れたプログラムが公開され、利用可能になっています。

しかし、そういったプログラムは、あらかじめ開発グループが実装した機能を、
開発グループがターゲットにしたマシン・OSで使うことしかできないのが普通
です。もちろん、オープンソースで公開されているものでは、原理的には
ソースコードを修正していろいろな機能を実装できるわけですが、巨大で複雑
なプログラムで、さらに特定のアーキテクチャ向けの最適化されたコードがで
るものを修正して動くようにするのは容易なことではないのは、やってみよう
と思ったことがある人は良くご存じのことかと思います。

なので、自分でプログラムを開発したいわけですが、それには
どうすればいいのか、というのが私たちの問題意識です。私たちが
提案する方法は、
特にMPIにかかわるような複雑な並列化とそのために必要なプログラムを、
実際に扱う系の記述や時間積分の方法の記述とを明確に分離することです。

明確に分離、と書くのは簡単ですが、実際にどのように実現するか、
十分に色々なことを表現するにはどうするか、計算速度がでるようにするには
どうするのか、と、いろいろな問題があります。実際に詳細にどうするか、は
連載に2回目以降で詳しく説明されますので、第一回である今回では、基本的
な考え方を紹介しようと思います。

\section{FDPSの基本的な考え方}

大規模並列化が可能で実行性能も高いMPI並列化粒子法シミュレー
ションコードを「だれでも」開発できるようにする、というのが、私たち
が FDPS (Framework for Developping Particle
Simulators) によって実現しようとしていることです。ここで、
「だれでも開発できる」というのは、具体的には、

\begin{itemize}

\item MPI を使った並列化はフレームワークが勝手にやる
\item OpenMP を使った並列化もフレームワークが勝手にやる
\item 長距離相互作用も短距離相互作用も、FDPSを使う人は粒子間の相互作用を
  計算する関数さえ用意すればよくてツリー法とかネイバーリストを使った
  高速化はフレームワークが勝手にやる

\end{itemize}
というふうに FDPS を作る、ということを意味します。

これを言い換えると、要するに、私たちが非常に単純な粒子系のプログラムを
書くと、

\begin{enumerate}

\item MPI や OpenMP を使った並列化はしない
\item 長距離力も短距離力もまずは全粒子からの力を単純に計算する
\item 相互作用計算ループも単純に書く。SIMD化とか特に意識しない

\end{enumerate}
というふうにしたいわけで、そういうふうに単純に作ったプログラムを、あま
り手をかけないで並列化・高速化できることが目標です。

そんなうまい話が本当にあるか、というわけですが、粒子系のシミュレーショ
ンプログラムはどういう構造をしているか、ということを考えてみると、基本
的な構造は以下のようになります。

\begin{enumerate}

\item 粒子の初期分布を作る(ファイルから読む・内部で生成する)
\item 粒子間相互作用を計算して、各粒子の加速度を求める
\item 速度をアップデートする(時間刻みの中央まで)
\item 位置をアップデートする  
\item ステップ 2 に戻る
\end{enumerate}  

ここでは時間積分にはリープフロッグを使うとしています。分子動力学では
リープフロッグや、それと等価な方法を使うことが多いと思います。ルンゲクッ
タ等を使うなら相互作用の計算を中間結果を使って行う必要がありますが、
いずれにしても相互作用を計算する部分とそれ以外をする部分があるのは同じ
です。


これを、MPI で並列化された、領域分割し、領域毎に自分の担当の粒子を持つ
プログラムにするとすれば、

\begin{enumerate}

\item 粒子の初期分布を作る(ファイルから読む・内部で生成する)
\item 領域分割のしかたを決める
\item 粒子を担当するプロセスが持つように転送する
\item 粒子間相互作用を計算して、各粒子の加速度を求める
\item 速度をアップデートする(時間刻みの中央まで)
\item 位置をアップデートする  
\item ステップ 2 に戻る
\end{enumerate}

となるでしょう。主な違いは、ステップ 2、3が入ること、ステップ4では、
各プロセスは自分の担当の粒子の加速度を計算するのに必要な情報を他の
プロセスからもらってこなければいけないことです。


ステップ1は、大規模並列では入力ファイルを並列に読む必要があったりして
若干面倒ですが、難しいものではありません。ステップ 5, 6 も、単純に全粒
子に対して、計算された加速度を使って時間積分の公式を適用するだけで、
特に難しいことはありません。積分公式がなにか違うものでも、全粒子に適用
する、ということには変わりありません。

FDPS がすることは、上のステップ 2, 3, 4 のそれぞれについて、そのための
ライブラリ関数を提供することです。ステップ2、3のためには、
ライブラリ関数は粒子を表すデータがどういうものかを知る必要がありますし、
また、MPIプロセス間で計算時間の差がでないようにするためになんらかの
計算負荷に関する情報を貰う必要もあります。

ライブラリが空間分割を決めるためには、例えば粒子の座標の配列を貰うこと
ができればいいですが、粒子を転送するためにはその粒子がどういうデータか
ら構成されるか、メモリにどう配置されているかをライブラリが知る必要があ
ります。

FDPS では、C++ 言語のクラスを使って、オブジェクトとして粒子を表現して
貰うことで、ユーザープログラム側で定義した粒子データの操作をライブラリ
側ですることを可能にしています。具体的には、粒子が「オブジェクト」であ
ることで、粒子の代入(コピー)が可能になり、また粒子の位置や電荷・質量を
返す関数を提供して貰うことで、長距離力のためのツリー構造を作ることもで
きます。さらに、相互作用を計算する関数は、粒子データ自体を受け取って相
互作用を計算する関数をユーザー側で定義することで、ライブラリ側は粒子デー
タの中身を知らないままで相互作用を計算する関数を呼び出せます。

FDPS はこのように C++ 言語の機能を使って実装されていますが、
現在はFortran 言語(2003以降)にも対応しています。近代的な Fortran では、
C++ のクラスに相当する「構造型」があり、また、C言語で作った構造体や関数
を Fortran 側からアクセスする方法も、言語仕様として定義されました。
これらの機能を利用して、Fortran で書かれたユーザープログラムからも
FDPS を利用可能にしています。

FDPS を使って粒子系シミュレーションプログラムを書くには

\begin{itemize}

\item 粒子を「構造体」で表現する
\item 粒子間相互作用をその粒子構造を引数にとる関数で書く
\end{itemize}

というFDPS側の要請に従う必要がありますが、それによって、ユーザープログ
ラムは MPI による並列化をほとんど意識することなく、MPI 並列でないプ
ログラムと同程度、ないしはより少ない手間で書くことができます。
また、コンパイル時のフラグを変えるだけで、MPIを使わないこともできるし、
OpeMP での並列化も、また MPI と OpenMPを組み合わせたハイブリッド並列も
できます。従って、ハイブリッド並列でないと大規模な実行ができない「京」
のような計算機でも高い性能を出すことができますし、その同じプログラムを
ノートパソコンで走らせることもできます。


もちろん、実際にはコンパイルされたプログラムでは FDPS 経由でMPI が呼ば
れています。ということは、ユーザーが書いたプログラムが複数のMPIプロセ
スを並列に実行されたり、1コアで実行されたりするわけです。MPI実行の場合
には、FDPSが勝手に領域分割をして粒子をプロセス間で交換します。

このため、ユーザープログラムから見ると知らないうちに勝手に粒子が入れ替
わって、粒子の数もFDPSが適当にいじっていることになります。もちろん、
MPIプロセス全部で見ると、どこかに粒子がありますが、調べなければどこに
あるかはわかりません。これは領域分割する粒子系プログラムでは必ず起こる
ことで、調べられるようにしておくためには、粒子データ構造体の中に粒子の
インデックスをいれておくのが普通のやり方です。

分子動力学計算では、原子間の結合を表現したいことがあります。FDPS では
今のところその辺は考えないで領域分割して粒子を移動するので、同じ分子に
属する原子が別のMPIプロセスに分かれたりします。分子結合等の表現につい
ては連載2回以降で扱います。

\section{FDPSの性能}

並列化をなんのためにするかというと、計算速度を速くするためです。ですから、
並列化はできますがあまり速くありません、となってしまっては FDPS の開発目標を実現した
とはいいがたいと考えます。

私たちが FDPS を開発するにあたって設定した目標は、

\begin{itemize}

\item ある程度粒子数が大きい計算では

\item 世界最大規模の計算機であっても、また粒子分布が著しく不均一でも

\item 理論的に実現できる限界に近い性能をだせる

\item さらに、アクセラレータをもうようなシステムでも高い実行効率を実現する

\end{itemize}

というものです。「ある程度」の意味は曖昧ですが、現在の通常の並列計算機
では MPI プロセスあたり数万粒子、計算時間では1タイムステップあたり数十
ミリ秒程度以上の、極端に少ない粒子数ではないところです。この程度以下に
なると、プロセス間通信の遅延時間等のために高い性能を出すのは現在の大規
模な汎用並列計算機では困難です。

上の目標を実現するためには、

\begin{itemize}

  \item MPI プロセス数が多い時に、理想に近いロードバランスを実現する。
    特に、計算時間が延びてしまうプロセスがないようにする
  \item 計算量のほとんどを占める、相互作用計算の計算カーネルを十分に最
    適化する。また、計算量が減るアルゴリズムを採用する
  \item 相互作用計算以外のところが足を引っ張らないよう、十分に効率的な
    アルゴリズムを採用し、実装も高速化する

\end{itemize}  

といったことが必要になります。実際にこれが実現できているか、ということ
ですが、まあまあのところまではきているのではないかと考えてています。

私たちのところで、分子動力学のシミュレーションの大規模なテストはまだ行っ
ていませんが、重力多体系や、SPH 法での流体シミュレーションでは、
相互作用計算が計算時間のほとんどになり、さらに「京」全ノードやその近く
までの大規模計算でも、粒子数をノード数に比例させてふやせば性能もノード
数にほぼ比例して上がる、という、十分に良い結果がえられています。
また、実行効率についても、理論ピークの 50\% 程度が実現できています。

なお、このような高い効率を出すためには、相互作用計算の関数は十分最適化
する必要があります。相互作用関数は現在のところユーザー側が提供するもの
であり、FDPS側で勝手に最適化はしません。但し、OpenMP による並列化まで
は FDPS 側でやりますので、ユーザー側では1コアでの性能の最適化、具体的
には SIMD 化や、多くの場合に性能ボトルネックになっている平方根や除算等
の高速化が主にするべきことになります。

この、相互作用関数についても、抽象度の高い記述から自動的に対象とするプ
ロセッサの命令セットやマイクロアーキテクチャに対して最適化したコードを
出すといったことはできなくはなさそうですが、対応しないといけないアーキ
テクチャが多く、またコンパイラの状況他色々なものに左右されそうなので、
まだ公開できるようなものはできていません。

\section{簡単な例}

と、いろいろ能書きを並べてきたわけですが、実際のところどんなコードを書
くとどんなことができるのか、を最後に紹介します。サンプルにするのは、
私が(かなり適当に)書いたLennard-Jones もどきの力で相互作用する分子動力
学計算コードです。

これは実際にダウンロード・コンパイル・実行することができます。チュートリアル
\footnote{https://github.com/FDPS/FDPS/blob/master/
  doc/doc\_tutorial\_cpp\_ja.pdf}
の手順に従って FDPS をダウンロード・展開したら、ディレクトリ
sample/c++/vdw\_test に移動して、 make; ./vdw\_test.out を実行すれば

{\scriptsize
\begin{verbatim}
******** FDPS has successfully begun. *****
./result/t-de.dat
./result/t-tcal.dat
MakeUniformCube: n_loc=512 and n_1d=8
np_ave=512
time:  0.0625000 energy error: +4.895409e-05
time:  0.1250000 energy error: +8.980742e-05
time:  0.1875000 energy error: +8.766743e-05
...
time: 10.0000000 energy error: +1.590795e-04
******** FDPS has successfully finished. ***
\end{verbatim}
}
というような出力がでるはずです。チュートリアルの、重力多体向けの記述を
参考に、Makefile を修正すれば、OpenMP や MPI での並列実行もできます。

このコードで、FDPS とのやりとりに必須な
\begin{itemize}
\item 粒子クラス
\item 相互作用関数
\end{itemize}  
を定義するコードを以下に示します。

{\scriptsize
\begin{verbatim}
class FPLJ{
public:
    PS::S64 id;
    PS::F64 mass, pot;
    PS::F64vec pos, vel, acc;
    PS::F64 search_radius;
    void clear(){
        acc = 0.0; pot = 0.0;
    }
    PS::F64 getRSearch() const{
        return this->search_radius;
    }
    PS::F64vec getPos() const {return pos;}
    void copyFromForce(const FPLJ & force){
        acc = force.acc;
        pot = force.pot;
    }
// (I/O 用関数を省略)
    void copyFromFP(const FPLJ & fp){ 
        mass = fp.mass;
        pos = fp.pos;
        id = fp.id;
        search_radius = fp.search_radius;
    }
};

void CalcForceFpFp(const FPLJ * ep_i,
                   const PS::S32 n_ip,
                   const FPLJ * ep_j,
                   const PS::S32 n_jp,
                   FPLJ * force){
    const PS::F64 r0 = 3;
    const PS::F64 r0sq = r0*r0;
    const PS::F64 r0inv = 1/r0;
    const PS::F64 r0invp6 = 1/(r0sq*r0sq*r0sq);
    const PS::F64 r0invp7 = r0invp6*r0inv;
    const PS::F64 foffset = -12.0*r0invp6
                       *r0invp7+6*r0invp7;
    const PS::F64 poffset = -13.0*r0invp6
                       *r0invp6+7*r0invp6;
    for(PS::S32 i=0; i<n_ip; i++){
        PS::F64vec xi = ep_i[i].pos;
        PS::F64vec ai = 0.0;
        PS::F64 poti = 0.0;
        PS::S64 idi = ep_i[i].id;
        for(PS::S32 j=0; j<n_jp; j++){
            if( idi == ep_j[j].id ) continue;
            PS::F64vec rij = xi - ep_j[j].pos;
            PS::F64 r2 = rij * rij;
            if (r2 < r0sq){
                PS::F64 r_inv = 1.0/sqrt(r2);
                PS::F64 r = r2*r_inv;
                PS::F64 r2_inv = r_inv * r_inv;
                PS::F64 r6_inv = r2_inv * r2_inv * r2_inv;
                PS::F64 r12_inv = r6_inv * r6_inv;
                poti += r12_inv - r6_inv-foffset*r+poffset;
                ai += (12*r12_inv*r2_inv - 6*r6_inv*r2_inv
                       + foffset*r_inv) * rij;
            }
        }
        force[i].acc += ai;
        force[i].pot += poti;
    }
}
\end{verbatim}
}
粒子クラスは FPLJ という名前で、id, mass, pos, vel, acc, pot,
search\_radius という変数を持ちます。粒子番号、質量、速度、加速度、ポ
テンシャル、ネイバーサーチの半径です。LJポテンシャル用には、短距離力計
算する FDPS のライブラリ関数を使います。これにはサーチ半径を渡すので、
それを与えるのが getRSearch です。また、 FDPS は getPos という名前の関
数が位置を返すことを期待します。さらに、 copyFrom なんとか、という関数
が2つありますが、これは、FDPS がキャッシュを考慮して、粒子に対していく
つかのデータ型を定義できるようになっているために、その間のコピーのため
に定義が必要になっています。このサンプルではデータ型1つですますのです
が、コピーする関数は必要です。 PS は FDPS で定義する名前空間、
S64, F64 はそれぞれ64ビットの整数、浮動小数点数、F64vec は3次元ベクト
ル型です。C++ のクラスを利用して3次元ベクトル型を定義しています。

相互作用を計算する関数は、CalcForceFpFp という名前です。この関数の引数
は、力を及ぼす粒子の配列、力を受ける粒子の配列、それぞれの粒子数、の4
つでいいはずです。実際には、力を受ける粒子のデータを入力と出力の2つに
わけているために引数が5個になっています。なお、現在のところ、FDPS では
力の対称性は使っていません。このため、計算量・通信量は短距離力について
は若干大きくなっています。今後、中点法のような、通信量・計算量を減らす
アルゴリズムの実装も検討します。


以下に、メインプログラムから FDPS を呼んで、領域分割し、粒子交換し、
近傍探索のためのツリー構造を作って粒子間相互作用を計算するところまでを
示します。

{\scriptsize
\begin{verbatim}
    PS::ParticleSystem<FPLJ> system_grav;
    system_grav.initialize();
    (ここで粒子データ設定)
    PS::DomainInfo dinfo;
    dinfo.initialize(coef_ema);
    dinfo.setBoundaryCondition(
          PS::BOUNDARY_CONDITION_PERIODIC_XYZ);
    dinfo.setPosRootDomain(
          PS::F64vec(-boxdh,-boxdh,-boxdh),
          PS::F64vec(boxdh,boxdh,boxdh));
    dinfo.collectSampleParticle(system_grav);
    dinfo.decomposeDomain();
    system_grav.exchangeParticle(dinfo);
    n_grav_loc = system_grav.getNumberOfParticleLocal();
    PS::TreeForForceShort<FPLJ, FPLJ, FPLJ>::Scatter
           tree_grav;
    tree_grav.initialize(n_grav_glb, theta,
              n_leaf_limit, n_group_limit);
    tree_grav.calcForceAllAndWriteBack(CalcForceFpFp,
              system_grav, dinfo);
\end{verbatim}
}
今回詳しい説明は省略しますが、基本的にはこれだけで、MPIを使った領域分
割から相互作用計算までが終わります。CalcForceFpFp が1コアで最適化され
ていれば、OpenMP や MPI で並列化しても高い性能がでます。

なお、GPGPU 等アクセラレータにも対応していますが、現在のところ相互作用
関数の書き方等が少し変わります。

次回以降では、より詳しい解説にはいっていきます。






% 謝辞
%\acknowledgement{○○氏に感謝します．}

% 参考文献
\begin{thebibliography}{9}
\bibitem{FDPS}
M. {Iwasawa},  A.  {Tanikawa}, N.  {Hosono}, K. {Nitadori},
T. {Muranushi}, and J. {Makino},
\textit{Publ. Astron. Soc. J.}, {\bf 68}, 54 (2016).
\end{thebibliography}

%------------------------------------------------------------------------
% 原稿ここまで
%------------------------------------------------------------------------

% 著者紹介出力
\profile




\end{document}

