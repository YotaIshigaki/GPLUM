
\section{FDPSの基本的な考え方}

大規模並列化が可能で実行性能も高いMPI並列化粒子法シミュレー
ションコードを「だれでも」開発できるようにする、というのが、私たち
が FDPS (Framework for Developping Particle
Simulators) によって実現しようとしていることです。ここで、
「だれでも開発できる」というのは、具体的には、FDPS を使うと

\begin{itemize}

\item MPI を使った並列化はFDPSが勝手にやる
\item OpenMP を使った並列化もFDPSが勝手にやる
\item 長距離相互作用も短距離相互作用も、FDPSを使う人は粒子間の相互作用を
  計算する関数さえ用意すればよくてツリー法とかネイバーリストを使った
  高速化はフレームワークが勝手にやる

\end{itemize}
ということを意味します。

私たちが非常に単純な粒子系のプログラムを
書く時には

\begin{enumerate}

\item MPI や OpenMP を使った並列化はしない
\item 長距離力も短距離力もまずは全粒子からの力を単純に計算する
\item 相互作用計算ループも単純に書く。SIMD化とか特に意識しない

\end{enumerate}
というふうにしたいわけで、そういうふうに単純に作ったプログラムを、あま
り手をかけないで並列化・高速化できることが目標です。

そんなうまい話が本当にあるか、というわけですが、粒子系のシミュレーショ
ンプログラムはどういう構造をしているか、ということを考えてみると、基本
的な構造は以下のようになります。

\begin{enumerate}

\item 粒子の初期分布を作る(ファイルから読む・内部で生成する)
\item 粒子間相互作用を計算して、各粒子の加速度を求める
\item 速度をアップデートする(時間刻みの中央まで)
\item 位置をアップデートする  
\item ステップ 2 に戻る
\end{enumerate}  

ここでは時間積分にはリープフロッグを使うとしています。粒子法では
リープフロッグや、それと等価な方法を使うことが多いと思います。ルンゲクッ
タ等を使うなら相互作用の計算を中間結果を使って行う必要がありますが、
いずれにしても相互作用を計算する部分とそれ以外をする部分があるのは同じ
です。


これを、MPI で並列化された、領域分割し、領域毎に自分の担当の粒子を持つ
プログラムにするとすれば、

\begin{enumerate}

\item 粒子の初期分布を作る(ファイルから読む・内部で生成する)
\item 領域分割のしかたを決める
\item 粒子を担当するプロセスが持つように転送する
\item 粒子間相互作用を計算して、各粒子の加速度を求める
\item 速度をアップデートする(時間刻みの中央まで)
\item 位置をアップデートする  
\item ステップ 2 に戻る
\end{enumerate}

となるでしょう。主な違いは、ステップ 2、3が入ること、ステップ4では、
各プロセスは自分の担当の粒子の加速度を計算するのに必要な情報を他の
プロセスからもらってこなければいけないことです。


ステップ1は、大規模並列では入力ファイルを並列に読む必要があったりして
若干面倒ですが、難しいものではありません。ステップ 5, 6 も、単純に全粒
子に対して、計算された加速度を使って時間積分の公式を適用するだけで、
特に難しいことはありません。積分公式がなにか違うものでも、全粒子に適用
する、ということには変わりありません。

FDPS がすることは、上のステップ 2, 3, 4 のそれぞれについて、そのための
ライブラリ関数を提供することです。ステップ2、3のためには、
ライブラリ関数は粒子を表すデータがどういうものかを知る必要がありますし、
また、MPIプロセス間で計算時間の差がでないようにするためになんらかの
計算負荷に関する情報を貰う必要もあります。

ライブラリが空間分割を決めるためには、例えば粒子の座標の配列を貰うこと
ができればいいですが、粒子を転送するためにはその粒子がどういうデータか
ら構成されるか、メモリにどう配置されているかをライブラリが知る必要があ
ります。

FDPS では、C++ 言語のクラスを使って、オブジェクトとして粒子を表現して
貰うことで、ユーザープログラム側で定義した粒子データの操作をライブラリ
側ですることを可能にしています。具体的には、粒子が「オブジェクト」であ
ることで、粒子の代入(コピー)が可能になり、また粒子の位置や電荷・質量を
返す関数を提供して貰うことで、長距離力のためのツリー構造を作ることもで
きます。さらに、相互作用を計算する関数は、粒子データ自体を受け取って相
互作用を計算する関数をユーザー側で定義することで、ライブラリ側は粒子デー
タの中身を知らないままで相互作用を計算する関数を呼び出せます。

FDPS はこのように C++ 言語の機能を使って実装されていますが、
現在はFortran 言語(2003以降)、C言語にも対応しています。近代的な Fortran では、
C++ のクラスに相当する「構造型」があり、また、C言語で作った構造体や関数
を Fortran 側からアクセスする方法も、言語仕様として定義されました。
これらの機能を利用して、C言語やFortran で書かれたユーザープログラムからも
FDPS を利用可能にしています。

FDPS を使って粒子系シミュレーションプログラムを書くには

\begin{itemize}

\item 粒子を「構造体」で表現する
\item 粒子間相互作用をその粒子構造を引数にとる関数で書く
\end{itemize}

というFDPS側の要請に従う必要がありますが、それによって、ユーザープログ
ラムは MPI による並列化をほとんど意識することなく、MPI 並列でないプ
ログラムと同程度、ないしはより少ない手間で書くことができます。
また、コンパイル時のフラグを変えるだけで、MPIを使わないこともできるし、
OpeMP での並列化も、また MPI と OpenMPを組み合わせたハイブリッド並列も
できます。従って、ハイブリッド並列でないと大規模な実行ができない「京」
のような計算機でも高い性能を出すことができますし、その同じプログラムを
ノートパソコンで走らせることもできます。


もちろん、実際にはコンパイルされたプログラムでは FDPS 経由でMPI が呼ば
れています。ということは、ユーザーが書いたプログラムが複数のMPIプロセ
スを並列に実行されたり、1コアで実行されたりするわけです。MPI実行の場合
には、FDPSが勝手に領域分割をして粒子をプロセス間で交換します。

このため、ユーザープログラムから見ると知らないうちに勝手に粒子が入れ替
わって、粒子の数もFDPSが変更することになります。もちろん、
MPIプロセス全部で見ると、どこかに粒子がありますが、調べなければどこに
あるかはわかりません。これは領域分割する粒子系プログラムでは必ず起こる
ことで、調べられるようにしておくためには、粒子データ構造体の中に粒子の
インデックスをいれておくのが普通のやり方です。

