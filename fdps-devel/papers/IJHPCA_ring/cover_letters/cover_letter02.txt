
Dear Dr. Bronis de Supinski

In response to the referee's report, we have revised our manuscript
and we resubmit our paper entitled "Implementation and Performance of
Barnes-Hut N-body algorithm on Extreme-scale Heterogeneous Many-core
Architectures". In the following, we describe changes we made in the
form of the reply to referee's comments.

Yours Sincerely,
Masaki Iwasawa
Daisuke Namekata
Ryo Sakamoto
Takashi Nakamura
Yasuyuki Kimura
Keigo Nitadori
Long Wang
Miyuki Tsubouchi
Jun Makino
Zhao Liu
Haohuan Fu
Guangwen Yang


To reviewer 1

> Reviewer: 1
>
> Comments to the Author
> In this paper, the authors develop their parallel Barnes-Hut tree code
> for two heterogeneous architectures, Sunway TaihuLight and PEZY-SC2.
> The work presents new algorithms to achieve high performance for such
> currently uncommon architectures and give great performance,
> and is deserving of publication in IJHPCA. However, underlying
> ideas and results are not well presented. Addressing the
> following issues are necessary.
>
>

Thank you for your careful reading and various comments on our
manuscript. In the following your comments, we describe changes we
made.

> Major comments: Section 3: It is hard to understand the
>specifications of the system used in this paper. Please use a table
>to explain the features of the three systems.

We added a table to specify the systems we used in section 3.

> Section 3, last paragraph: "Just one of these ... previously known
> parallelization algorithms". For item number 1, 3, 4, and 5, I agree
> with this statement. Regarding 2, the well known algorithm by Barnes
> 1990, which is used in the author's code, can significantly reduce
> the required memory B/F if the cache memory size is enough. For the
> application (Saturn's ring) in this paper, how severe is very small
> memory B/F?

For simple parallel tree algorithm, we need to access all particle
data about 50 times per step and the size of a particle is about
64B. Thus the amount of memory access is roughly 3200n.  On the other
hand, for the force calculation, in the Saturn's ring simulation, the
size of the interaction list is about 2000 and the number of
operations per interaction is about 40. Thus the total number of
floating operations per step is 80000n. Thus B/F should be greater
than 0.04. We added these discussions in section 3.

> Section 4.2: the persistent interaction list method. This method
> should give additional force errors depending on the distribution of
> particles. To reduce the error, are there any differences in time
> stepping and tree opening criterion between with and without sharing
> interaction lists over multiple timesteps?

As you pointed out, the persistent interaction list method gives the
additional force error from changing the distribution of
particles. This force error is not large in our simulation, because
the random and shear motion is very small. However, the time
integration error would become large because the force error is
correlated with time. Roughly speaking, since the force error is
proportional to time, the integration error is proportional to time
squared (please see figure XXX in section YYY).

To reduce this integration error, we developed a new method to use the
same interaction list constructed at the advanced time. For example,
if we use the same interaction list for 64 steps, at first, we
construct the interaction list after 32 steps by using predicted
positions of particles and use this list for 64 steps. To do so, the
integration error generated for the first 32 steps would be canceled
out with that generated for the last 32 steps. We implemented this
method to our code and confirmed this works very well (please see
figure XXX in section YYY). As a result, by using this method we do
not have to make timestep and opening criterion small.

Unfortunately, we did not use this method for the performance
measurements. However, the additional cost is very small because
particles are almost on the circular orbits and easily predict their
positions. Thus even if we use this method, the performance results
would not change. We added above descriptions in sections 4.2 and XXX
as a new section.



> Figure 3:
> Why do time per one step on TaihuLight and Shoubu-B show up-down?
>

This up-down is caused by variation in the size of the interaction
list per particle, changing the number of particles. In addition,
every factor of four in the number of particles, the size of the
interaction list per particle is almost the same because the particles
are almost on 2D plane and the tree structure change in a self-similar
way. Thus the up-down features clearly seen. We added these
descriptions in section 5.2.

>
> Minor comments:
>
> Abstract: "global" simulation
> The meaning of "global" is unclear.
>

We meant whole ring simulation by "global" simulation. We clearly
mentioned this point in abstract and section 1.


> Section 1 5th paragraph: "OpenACC results in rather poor performances
> in the case of TaihuLight".
> References are necessary.
>

Thank you for your comments. We added a reference, Cai et al.(2018),
in 5th paragraph in section 1.


> "and thus the current system ... Xeon-D CPU".
> Typo, without a verb.
>

Thank you for finding typo. We corrected it.

> Section 2, 1st paragraph: Clarify the meaning of "more Centaurs".
>

Centaurs are minor planets between Saturn and Uranus. We added this
description in section 2.


> Section 2, 6th paragraph: "Almost all previous ... only for 10
> orbital periods".  This paragraph and the last two paragraphs are
> near duplicates.

Thank you for your comments. we combined these paragraphs into one
paragraph.


> Section 2, 7th paragraph: "or around 10^12 particle-steps".
> I do not understand where the number 10^12 comes. Do the authors
> implicitly assume 1,000 steps are necessary for an orbit? If so, please
> specify.
>

Thank you for your comments. Yes, we meant 1,000 steps are necessary
for an orbit. We added this description in section 2.


> Section 4.2, 2nd paragraph: "However, on almost all modern implementation".
> References are necessary.
>

Thank you for your comments. We added the references, Bedorf et
al.(2014) and ishiyama et al. (2012), in this sentence.


> Section 4.5, 3rd paragraph:
> How can this algorithm reduce the communication cost? Please describe
> a typical reduction rate.
>

For many supercomputers, MPI_Alltoall is not highly optimized. For
example, on TaihuLight, the time for MPI_Alltoall with a short message
size among 10000 processes is a several minutes. On the other hand,
that for Allgather is about ten millisecond. Thus the reduction rate
is roughly 10000. Thus to avoid to use MPI_Alltoall is necessary. We
added these descriptions in section 4.5.


> Section 4.6, item number 3: Typo "to the the CPE". .
>

Thank you for your comments. We corrected it.


> Section 4.7, 1st paragraph: "does not give very good performance".
> It would be helpful to mention the performance qualitatively.
>

In the compiler generated code, there is no loop unrolling and
scheduling is not good. As a results, the achieved performance around
10 %. We added these description in section 4.7.


> Section 5, last paragraph:
> Why is this statement made here, what does it mean? What is your intent to compare with
> results from old and "smaller" supercomputers?
>

In this paragraph, we would like to state that by using our
algorithms, we can achieve almost the same performance efficiency on
modern, "bigger" and low B/F supercomputers as on old, "smaller" and
high B/F supercomputers. We clearly mentioned this point in section 5.


> Figure 5: It would be helpful to plot lines showing idealized scaling.
>

Thank you for your comments. We added the line showing idealized
scaling in figure 5.


> Section 5.1, 3rd paragraph: "For particle-tree-node, interaction, we
> used center-of-mass approximation".
> When higher-order terms (e.g., quadrupole) are calculated, what
> performance gain should be expected?
>

In our simulations, the energy error for the run with center of mass
approximation (effectively dipole moment) with theta=0.5 is almost the
same as that for the run with up to quadrupole moment with
theta=1.0. If we use theta=1.0, the size of the interaction list per
particle is reduced by 30% compared with the case of theta=0.5 and the
time to solution would be reduced by 30 %. However, even if we use
theta=1.0, the size of the interaction list is enough long so that the
processors are always busy. Thus the performance would not be
changed. We added these descriptions in section 5.1.


> Section 6, 3rd paragraph:
> What does "Athread call" mean?
>

Atherad is a thread library to use CPEs in parallel. We added this
description in section 6.



To Reviewer 2

> Reviewer: 2
>
> Comments to the Author
> This paper is basically well written and presents useful information on the parallel Barnes-Hut algorithm on heterogeneous many-core systems (such as software frameworks, performance measurements, and new algorithms).

Thank you for your careful reading and various comments on our
manuscript. In the following your comments, we describe changes we
made.

>
> However, I have several concerns.
>
> (1) Section 4.1 introduces several new algorithms. However, their individual effects (improvements) or even combined effects are not shown quantitatively in the paper except for "less than 1%" in Section 4.2. For example, in Section 4.5, "the reduction ... was quite significant" is not a quantitative evaluation.
>

In the following, we quantitatively describe individual effects of the
methods described in section 4.

* For using cylindrical coordinate (section 4.3).

  The amount of data size of communication is roughly proportional to
  the surface areas of domains. For example, if we use 160K (5x32000)
  nodes, the total surface area of domains in Cartesian coordinate is
  about 20 times larger than that in Cylindrical coordinate. Thus the
  amount of data size of the communication is reduced by a factor of
  20. We added these descriptions in section 4.3.

* For coordinate rotation (section 4.4).

  In our simulation, each particle moves about 0.4 radian in the theta
  direction for 64 combined steps.  If we use 160K (5x32000) nodes,
  without coordinate rotation, all particles in all nodes need to be
  sent other nodes. However, in the rotating frame, most of particles
  seem to move very slow. Even the fastest particles moves only 2e-4
  radian in the theta direction for 64 combined steps. As a result, by
  using coordinate rotation method, the only 5 % of particles need to
  be sent to other domains. Thus we can reduce communication cost by a
  factor of 20. We added these descriptions in section 4.4.


* Elimination of all-to-all communication (section 4.5)

  For many supercomputers, MPI_Alltoall is not highly optimized. For
  example, on TaihuLight, the time for MPI_Alltoall with a short
  message size among 10000 processes is a several minutes. On the
  other hand, that for Allgather is about ten milliseconds. Thus we
  need to avoid MPI_Alltoall. We added these descriptions in section
  4.5.


* Load balance among computing cores (section 4.6)

  Without this method, the typical variation of the force calculation
  cost among all CPEs is 40%. On the other hand, with this method, the
  variation is a few percent. As a result, by using this method, the
  time for the force calculation is reduced by 10 %. We added these
  descriptions in section 4.6.

* Interaction Kernel (section 4.7)

  In the compiler generated code, there is no loop unrolling and the
  scheduling is not good. As a results, the achieved performance
  around 10 %. Thus our kernel written in the assembly language is
  five times faster than the kernel generated by the compiler.


> (2) It is unclear how significantly persistent interaction lists make the calculation inaccurate compared to the pre-existing Barnes-Hut approximation. For example, if we need to make interaction lists 20% long in order to achieve the equivalent degree of approximation, the 20% increased floating operations are just overheads.

As we will describe below, we do not have to make the interaction list
long, if we use a new method to reduce the additional error generated
by the persistent interaction list method.

As you pointed out, the persistent interaction method gives the
additional force errors. In our simulations this force error is not
significant because the random motion and shear motion is very
small. However, we found that the integration error could become large
because the force error is correlated with time. Roughly speaking,
since the additional force error is proportional to time, the
integration error is proportional to time squared (please see figure
XXX in section YYY).

To reduce this integration error, we developed a new method to use the
same interaction list constructed at the advanced time. For example,
if we use the same interaction list for 64 steps, at first, we
construct the interaction list after 32 steps by using predicted
positions of particles and reuse it for 64 steps. To do so, the
integration error generated for the first 32 steps would be canceled
out with that generated for the last 32 steps. We implemented this
method to our code and confirmed this works very well (please see
figure XXX in section YYY).

Unfortunately, we did not use this method for the performance
measurements. However, the additional cost is very small because
particles are almost on the circular orbits and easily predict their
positions. Thus even if we use this method, the performance results
would not change. We added above descriptions in sections 4.2 and XXX
as a new section.


>Note that Hamada et al. 2009 seems not to count increased particle-particle interactions for GPU (compared to the corresponding particle-cell interactions for CPU) for their title's "42 TFlops".

We do not think that it worth to compare the size of the interaction
list for modern CPUs, because the minimum size of the interaction list
per particle is achieved when the interaction list is not shared for
multiple particles (i.e. without Barnes vectorized scheme). If we do
not use Barnes vectorized scheme, the typical size of the interaction
list per particle is about 1500, on the other hand, in our
simulations, the size is about 2000. Thus if we use the size of the
interaction list without Barnes vectorized scheme to measure the
performance, the performances are 36, 8 PF and 0.75 PF on TaihuLight,
Gyoukou and Shoubu System B. We added these descriptions in section
5.2.


> (3) In Section 4.7, it is unclear how significantly the use of single precision operations make the calculation inaccurate compared to the pre-existing Barnes-Hut approximation.
>

According to McMillan and Aarseth (1993), the relative force error by
introducing center-of-mass approximation (effectively dipole moment
approximation) with theta=0.5 is about 0.1%. This force error can be
expressed by 10 bits. Thus the error from using single precision
operations is much smaller than that from the Barnes-Hut
approximation. We added this point in section 4.7.



> (4) Since Section 5.1 shows that "For PEZY-SC2 based systems we used the same operation count as we used for TaihuLight", does this mean that performances such as 1.01PF and efficiency such as 35.5% are more than real values?

PEZY-SC2 has special function units (SFU) for reciprocal square root
operation and we used it for the force calculation. However, the
latency of SFU is very large. If we considered this latency, the
efficiency for PEZY-SC2 based system would increase by several
percent. We added these description in section 5.1.
